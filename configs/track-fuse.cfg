#
# Example configuration file for sparse pose tracking and sparse pose fusion.
#


# dataRoot and testRoot define where the data come from.
# The full path will be <dataRoot>/<testRoot>/
dataRoot = "/data/"               # dataRoot can also be specified in your ~/.mc_dev.common.cfg file
testRoot = "bioCV/27/P27_RUN_01/"


# Points to where the pose data comes from for each camera
# <dataRoot>/<testRoot>/poseSource
# The exact nature of what gets loaded depends on the 
# specified skeleton type.
# For OpenPose and AlphaPose, these are the directories
# containing per-frame .json files.
poseSources = (
                 "openpose_output_00/",
                 "openpose_output_01/",
                 "openpose_output_02/",
                 "openpose_output_03/",
                 "openpose_output_04/",
                 "openpose_output_05/",
                 "openpose_output_06/",
                 "openpose_output_07/",
                 "openpose_output_08/"
              );

# Which pose detector produced the per frame, per camera
# sparse pose detections?
#  - open : OpenPose
#  - alpha: AlphaPose
skelType = "open"

# For each pose source, specify a calibration file.
# The order of this must match the order of pose 
# sources.
# again, these paths are relative to the testRoot.
calibFiles = (
                "../00.mp4-mocAligned.calib",
                "../01.mp4-mocAligned.calib",
                "../02.mp4-mocAligned.calib",
                "../03.mp4-mocAligned.calib",
                "../04.mp4-mocAligned.calib",
                "../05.mp4-mocAligned.calib",
                "../06.mp4-mocAligned.calib",
                "../07.mp4-mocAligned.calib",
                "../08.mp4-mocAligned.calib"
             );

#
# Optional:
# If visualisation is enabled, you can specify the 
# image sources. These can be video files
# or directories of image files.
#
imgSources = (
                "00.mp4",
                "01.mp4",
                "02.mp4",
                "03.mp4",
                "04.mp4",
                "05.mp4",
                "06.mp4",
                "07.mp4",
                "08.mp4"
             );

#
# This defines the volume of the observed area.
# Typically, the units to use are millimetres
# but it will depend upon your calibration files.
#
minX = -1500.0;
maxX =  1500.0;
minY = -4000.0;
maxY =  4000.0;
minZ =     0.0;
maxZ =  2000.0;

#
# What direction is "up" in the world?
#
upDir = "z";


#
# If your video happens to have a lot of 
# empty frames at the start, then you can skip them
# by saying what frame to start working on.
#
firstFrame = 1200;

#
# The assocFile is where trackSparsePose will save its 
# cross-camera associations and person tracks. It is thus 
# also where fuseSparsePose will load that data from.
#
assocFile     = "op.assoc";

#
# This is the directory (relative to testRoot) where 
# fuseSparsePoses will output its reconstructions
#
reconDir      = "op-recon";


#
# fuseSparsePoses will save its output as a simple text file,
# but can also output to .c3d files. You will need to enable 
# the .c3d output if you want to use the tools from mc_opensim
#
saveC3D       = true;

#
# We had a use case where we wanted to sync the .c3d output against 
# some marker based mocap data, and hence, created the option here.
# You can probably wont need this.
#
#C3DOffsetFile = "frameOffset";


# ------------------------------------------------------------------------
# 
# Settings for occupancy based cross-camera association (trackSparsePoses)
#
# ------------------------------------------------------------------------


# Divide the observed area into square cells of this size
# If we pad the cells slightly so that they overlap each other,
# then our occupancy map gets a bit blurry and more robust.
cellSize = 100.0;
cellPadding   = 5.0;

# Each cell has some height in the world along the up-axis.
planeLow  =    0.0;
planeHigh = 2000.0;

# What is the minimum number of cameras that a cell must be seen it
# for us to use it?
minVisibility =  3;

# Do we normalise the occupancy score based on the number of cameras
# that can view a cell?
useVisibility = false;

# What is the minimum occupancy for a cell to be considered occupied?
# 0->1
detectionThreshold = 0.6;

# After detecting occupancy peaks in each frame we then want to 
# connect them together through time. We do this by connecting 
# all the peaks in a graph and then finding the spanning tree(s).

# What is the maximum distance between peaks to see them connected (pixels)
distanceThreshold = 200.0;

# How many near peaks should a peak connect to?
numNearPeaks = 50;


# -------------------------------------------------------------------------
#
# Settings for pose fusion (fuseSparsePoses)
#
# -------------------------------------------------------------------------

# What is the minimum confidence of a keypoint for us to consider fusing it
# into a 3D point?
minJointConfidence = 0.4

# For the RANSAC process, that is the distance threshold for a detection
# to cound as an inlier?
singleJointDistanceThreshold = 10.0

# For a single keypoint, how many inliers do we need to create a 3D keypoint?
singleJointMinInliers = 3


#
# What method should we use for deciding on left/right labels?
#
# votes    : Majority voting between the inliers of a keypoint.
# yplanepos: The person is always facing towards +ve y
# yplaneneg: The person is always facing towards -ve y
# xplanepos:
# xplaneneg:
# zplanepos:
# zplaneneg:
# 
leftRightDecision = "votes"


# Run the debug visualiser. It probably is not that useful to you.
# Note that fusion results can be visualised using either projectMocap
# or compareMocap tools.
visualise = false;

